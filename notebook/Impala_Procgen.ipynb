{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Impala_Procgen.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Enable cuda\n",
        "\n",
        "In case of google colab.\n",
        "To enable GPU hardware accelerator, just go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
        "\n"
      ],
      "metadata": {
        "id": "BlV-GodSKuVh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M36ZlfCrKZn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d76f446-6858-4300-a8e9-ebd325ca8670"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install procgen dependencies\n",
        "!pip install procgen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT8Qh9f3TDKO",
        "outputId": "7ececa44-4223-4fa2-a2ca-13b224378fee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: procgen in /usr/local/lib/python3.7/dist-packages (0.10.4)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from procgen) (3.4.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from procgen) (1.19.5)\n",
            "Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from procgen) (0.17.3)\n",
            "Requirement already satisfied: gym3<1.0.0,>=0.3.3 in /usr/local/lib/python3.7/dist-packages (from procgen) (0.3.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.4.1)\n",
            "Requirement already satisfied: imageio-ffmpeg<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (0.3.0)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (2.13.5)\n",
            "Requirement already satisfied: moderngl<6.0.0,>=5.5.4 in /usr/local/lib/python3.7/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (5.6.4)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.15.0)\n",
            "Requirement already satisfied: glfw<2.0.0,>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.12.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.21)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.7/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (8.4.0)\n",
            "Requirement already satisfied: glcontext<3,>=2 in /usr/local/lib/python3.7/dist-packages (from moderngl<6.0.0,>=5.5.4->gym3<1.0.0,>=0.3.3->procgen) (2.3.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<1.0.0,>=0.15.0->procgen) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the training parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "vpUmGYcSKs3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "million_steps = 10\n",
        "num_levels = 10000\n",
        "game = 'coinrun'\n",
        "# Model: 1 - impala, 2 - leaky impala, 3 - 5 blocks impala\n",
        "model = 1\n",
        "# Data augmentation strategy, 0=identity, 1=crop, 2=translate, 3=cutout, 4=colormix, 5=random sequences of all\n",
        "training_aug = 0\n",
        "num_features = 256\n",
        "# Validation augmentation strategy. Can be 0-4 or not specified. When specified, this strategy will be removed from the random rounds of Arg5.\n",
        "validation_aug = 0\n"
      ],
      "metadata": {
        "id": "dPYO2EoMM-QE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using define parameters"
      ],
      "metadata": {
        "id": "2QhoAc-HM_OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from random import randrange\n",
        "\n",
        "\n",
        "total_steps = (int(million_steps) * 1e6)\n",
        "print(\"Total steps: \" + str(total_steps))\n",
        "# lowering envs. in colab. Runs out of memory with 64.\n",
        "num_envs = 16\n",
        "\n",
        "print(\"Num levels: \" + str(num_levels))\n",
        "num_levels = int(num_levels)\n",
        "\n",
        "eval_frequency_min = 0.33e6\n",
        "eval_frequency_max = 2e6\n",
        "num_steps = 256\n",
        "num_epochs = 3\n",
        "batch_size = 1024  #Use lower for low VRAM\n",
        "eps = .2\n",
        "grad_eps = .5\n",
        "value_coef = .5\n",
        "entropy_coef = .01\n",
        "gamma = 0.99\n",
        "\n",
        "print(\"Game: \" + game)\n",
        "env_name = game\n",
        "\n",
        "print(\"Model: \" + str(model))\n",
        "model = int(model)\n",
        "\n",
        "print(\"Augmentation mode: \" + str(training_aug))\n",
        "augmentationMode = int(training_aug)\n",
        "\n",
        "print(\"nr_features: \" + str(num_features))\n",
        "nr_features = int(num_features)\n",
        "\n",
        "useHoldoutAugmentation = False\n",
        "\n",
        "print(\"Augmentation mode validation: \" + str(validation_aug))\n",
        "augmentationModeValidation = int(validation_aug)\n",
        "useHoldoutAugmentation = True\n",
        "\n",
        "randomAugmentation = False\n",
        "\n",
        "if augmentationMode > 4:\n",
        "    randomAugmentation = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpvtUW0kNXWm",
        "outputId": "4443c59c-240e-4480-e35f-a8b25329490f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total steps: 10000000.0\n",
            "Num levels: 10000\n",
            "Game: coinrun\n",
            "Model: 1\n",
            "Augmentation mode: 0\n",
            "nr_features: 256\n",
            "Augmentation mode validation: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load different IMPALA models\n"
      ],
      "metadata": {
        "id": "iV6umTJsPvin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "def xavier_uniform_initializer(module, gain=1.0):\n",
        "    if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "        nn.init.xavier_uniform_(module.weight.data, gain)\n",
        "        nn.init.constant_(module.bias.data, 0)\n",
        "    return module\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 a_fn = nn.ReLU()):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.activation = a_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.LeakyReLU()(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.conv3(out)\n",
        "        return out + x\n",
        "\n",
        "\n",
        "class ImpalaBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, a_fn = nn.ReLU()):\n",
        "        super(ImpalaBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.res1 = ResidualBlock(out_channels, a_fn = a_fn)\n",
        "        self.res2 = ResidualBlock(out_channels, a_fn = a_fn)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ImpalaModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels, out_channels=256,\n",
        "                 **kwargs):\n",
        "        super(ImpalaModel, self).__init__()\n",
        "        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16)\n",
        "        self.block2 = ImpalaBlock(in_channels=16, out_channels=32)\n",
        "        self.block3 = ImpalaBlock(in_channels=32, out_channels=64)\n",
        "        self.fc = nn.Linear(in_features=64 * 8 * 8, out_features=out_channels)\n",
        "\n",
        "        self.output_dim = out_channels\n",
        "        self.apply(xavier_uniform_initializer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = Flatten()(x)\n",
        "        x = self.fc(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ItQ9upslPyV_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LeakyImpalaModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels, out_channels = 256,\n",
        "                 **kwargs):\n",
        "        super(LeakyImpalaModel, self).__init__()\n",
        "        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=16, a_fn = nn.LeakyReLU())\n",
        "        self.block2 = ImpalaBlock(in_channels=16, out_channels=32, a_fn = nn.LeakyReLU())\n",
        "        self.block3 = ImpalaBlock(in_channels=32, out_channels=64, a_fn = nn.LeakyReLU())\n",
        "        self.fc = nn.Linear(in_features=64 * 8 * 8, out_features=out_channels)\n",
        "\n",
        "        self.output_dim = out_channels\n",
        "        self.apply(xavier_uniform_initializer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = nn.LeakyReLU()(x)\n",
        "        x = Flatten()(x)\n",
        "        x = self.fc(x)\n",
        "        x = nn.LeakyReLU()(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "H3VvYeQ0Qh-X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FiveBlocksImpala(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels, out_channels = 256,\n",
        "                 **kwargs):\n",
        "        super(FiveBlocksImpala, self).__init__()\n",
        "        self.block1 = ImpalaBlock(in_channels=in_channels, out_channels=8)\n",
        "        self.block2 = ImpalaBlock(in_channels=8, out_channels=16)\n",
        "        self.block3 = ImpalaBlock(in_channels=16, out_channels=32)\n",
        "        self.block4 = ImpalaBlock(in_channels=32, out_channels=64)\n",
        "        self.block5 = ImpalaBlock(in_channels=64, out_channels=128)\n",
        "        self.fc = nn.Linear(in_features=128 * 4, out_features=out_channels)\n",
        "\n",
        "        self.output_dim = out_channels\n",
        "        self.apply(xavier_uniform_initializer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = self.block5(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = Flatten()(x)\n",
        "        x = self.fc(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LrRjgUTwS1ff"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load utils."
      ],
      "metadata": {
        "id": "-80gb5q5TsZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import contextlib\n",
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from gym import spaces\n",
        "import time\n",
        "from collections import deque\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "from procgen import ProcgenEnv\n",
        "from collections import deque\n",
        "\n",
        "\"\"\"\n",
        "Utility functions for the deep RL projects that I supervise in 02456 Deep Learning @ DTU.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def make_env(\n",
        "\tn_envs=32,\n",
        "\tenv_name='starpilot',\n",
        "\tstart_level=0,\n",
        "\tnum_levels=100,\n",
        "\tuse_backgrounds=True,\n",
        "\tnormalize_obs=False,\n",
        "\tnormalize_reward=True,\n",
        "\tseed=0,\n",
        "\tgamma=0.99\n",
        "\t):\n",
        "\t\"\"\"Make environment for procgen experiments\"\"\"\n",
        "\tset_global_seeds(seed)\n",
        "\tset_global_log_levels(40)\n",
        "\tenv = ProcgenEnv(\n",
        "\t\tnum_envs=n_envs,\n",
        "\t\tenv_name=env_name,\n",
        "\t\tstart_level=start_level,\n",
        "\t\tnum_levels=num_levels,\n",
        "\t\tdistribution_mode='easy',\n",
        "\t\tuse_backgrounds=use_backgrounds,\n",
        "\t\trestrict_themes=not use_backgrounds,\n",
        "\t\trender_mode='rgb_array',\n",
        "\t\trand_seed=seed\n",
        "\t)\n",
        "\tenv = VecExtractDictObs(env, \"rgb\")\n",
        "\tenv = VecNormalize(env, ob=normalize_obs, ret=normalize_reward, gamma = gamma)\n",
        "\tenv = TransposeFrame(env)\n",
        "\tenv = ScaledFloatFrame(env)\n",
        "\tenv = TensorEnv(env)\n",
        "\t\n",
        "\treturn env\n",
        "\n",
        "\n",
        "class Storage():\n",
        "\tdef __init__(self, obs_shape, num_steps, num_envs, gamma=0.99, lmbda=0.95, normalize_advantage=True):\n",
        "\t\tself.obs_shape = obs_shape\n",
        "\t\tself.num_steps = num_steps\n",
        "\t\tself.num_envs = num_envs\n",
        "\t\tself.gamma = gamma\n",
        "\t\tself.lmbda = lmbda\n",
        "\t\tself.normalize_advantage = normalize_advantage\n",
        "\t\tself.reset()\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tself.obs = torch.zeros(self.num_steps+1, self.num_envs, *self.obs_shape)\n",
        "\t\tself.action = torch.zeros(self.num_steps, self.num_envs)\n",
        "\t\tself.reward = torch.zeros(self.num_steps, self.num_envs)\n",
        "\t\tself.done = torch.zeros(self.num_steps, self.num_envs)\n",
        "\t\tself.log_prob = torch.zeros(self.num_steps, self.num_envs)\n",
        "\t\tself.value = torch.zeros(self.num_steps+1, self.num_envs)\n",
        "\t\tself.returns = torch.zeros(self.num_steps, self.num_envs)\n",
        "\t\tself.advantage = torch.zeros(self.num_steps, self.num_envs)\n",
        "\t\tself.info = deque(maxlen=self.num_steps)\n",
        "\t\tself.step = 0\n",
        "\n",
        "\tdef store(self, obs, action, reward, done, info, log_prob, value):\n",
        "\t\tself.obs[self.step] = obs.clone()\n",
        "\t\tself.action[self.step] = action.clone()\n",
        "\t\tself.reward[self.step] = torch.from_numpy(reward.copy())\n",
        "\t\tself.done[self.step] = torch.from_numpy(done.copy())\n",
        "\t\tself.info.append(info)\n",
        "\t\tself.log_prob[self.step] = log_prob.clone()\n",
        "\t\tself.value[self.step] = value.clone()\n",
        "\t\tself.step = (self.step + 1) % self.num_steps\n",
        "\n",
        "\tdef store_last(self, obs, value):\n",
        "\t\tself.obs[-1] = obs.clone()\n",
        "\t\tself.value[-1] = value.clone()\n",
        "\n",
        "\tdef compute_return_advantage(self):\n",
        "\t\tadvantage = 0\n",
        "\t\tfor i in reversed(range(self.num_steps)):\n",
        "\t\t\tdelta = (self.reward[i] + self.gamma * self.value[i+1] * (1 - self.done[i])) - self.value[i]\n",
        "\t\t\tadvantage = self.gamma * self.lmbda * advantage * (1 - self.done[i]) + delta\n",
        "\t\t\tself.advantage[i] = advantage\n",
        "\n",
        "\t\tself.returns = self.advantage + self.value[:-1]\n",
        "\t\tif self.normalize_advantage:\n",
        "\t\t\tself.advantage = (self.advantage - self.advantage.mean()) / (self.advantage.std() + 1e-9)\n",
        "\n",
        "\tdef get_generator(self, batch_size=1024):\n",
        "\t\titerator = BatchSampler(SubsetRandomSampler(range(self.num_steps*self.num_envs)), batch_size, drop_last=True)\n",
        "\t\tfor indices in iterator:\n",
        "\t\t\tobs = self.obs[:-1].reshape(-1, *self.obs_shape)[indices].cuda()\n",
        "\t\t\taction = self.action.reshape(-1)[indices].cuda()\n",
        "\t\t\tlog_prob = self.log_prob.reshape(-1)[indices].cuda()\n",
        "\t\t\tvalue = self.value[:-1].reshape(-1)[indices].cuda()\n",
        "\t\t\treturns = self.returns.reshape(-1)[indices].cuda()\n",
        "\t\t\tadvantage = self.advantage.reshape(-1)[indices].cuda()\n",
        "\t\t\tyield obs, action, log_prob, value, returns, advantage\n",
        "\n",
        "\tdef get_reward(self, normalized_reward=True):\n",
        "\t\tif normalized_reward:\n",
        "\t\t\treward = []\n",
        "\t\t\tfor step in range(self.num_steps):\n",
        "\t\t\t\tinfo = self.info[step]\n",
        "\t\t\t\treward.append([d['reward'] for d in info])\n",
        "\t\t\treward = torch.Tensor(reward)\n",
        "\t\telse:\n",
        "\t\t\treward = self.reward\n",
        "\t\t\n",
        "\t\treturn reward.mean(1).sum(0)\n",
        "\n",
        "\n",
        "def orthogonal_init(module, gain=nn.init.calculate_gain('relu')):\n",
        "\t\"\"\"Orthogonal weight initialization: https://arxiv.org/abs/1312.6120\"\"\"\n",
        "\tif isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "\t\tnn.init.orthogonal_(module.weight.data, gain)\n",
        "\t\tnn.init.constant_(module.bias.data, 0)\n",
        "\treturn module\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Helper functions that set global seeds and gym logging preferences\n",
        "\"\"\"\n",
        "\n",
        "def set_global_seeds(seed):\n",
        "\ttorch.backends.cudnn.deterministic = True\n",
        "\ttorch.backends.cudnn.benchmark = False\n",
        "\ttorch.manual_seed(seed)\n",
        "\ttorch.cuda.manual_seed_all(seed)\n",
        "\tnp.random.seed(seed)\n",
        "\trandom.seed(seed)\n",
        "\n",
        "\n",
        "def set_global_log_levels(level):\n",
        "\tgym.logger.set_level(level)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Copy-pasted from OpenAI to obviate dependency on Baselines. Required for vectorized environments.\n",
        "You will never have to look beyond this line.\n",
        "\"\"\"\n",
        "\n",
        "class AlreadySteppingError(Exception):\n",
        "\t\"\"\"\n",
        "\tRaised when an asynchronous step is running while\n",
        "\tstep_async() is called again.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tmsg = 'already running an async step'\n",
        "\t\tException.__init__(self, msg)\n",
        "\n",
        "\n",
        "class NotSteppingError(Exception):\n",
        "\t\"\"\"\n",
        "\tRaised when an asynchronous step is not running but\n",
        "\tstep_wait() is called.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tmsg = 'not running an async step'\n",
        "\t\tException.__init__(self, msg)\n",
        "\n",
        "\n",
        "class VecEnv(ABC):\n",
        "\t\"\"\"\n",
        "\tAn abstract asynchronous, vectorized environment.\n",
        "\tUsed to batch data from multiple copies of an environment, so that\n",
        "\teach observation becomes an batch of observations, and expected action is a batch of actions to\n",
        "\tbe applied per-environment.\n",
        "\t\"\"\"\n",
        "\tclosed = False\n",
        "\tviewer = None\n",
        "\n",
        "\tmetadata = {\n",
        "\t\t'render.modes': ['human', 'rgb_array']\n",
        "\t}\n",
        "\n",
        "\tdef __init__(self, num_envs, observation_space, action_space):\n",
        "\t\tself.num_envs = num_envs\n",
        "\t\tself.observation_space = observation_space\n",
        "\t\tself.action_space = action_space\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef reset(self):\n",
        "\t\t\"\"\"\n",
        "\t\tReset all the environments and return an array of\n",
        "\t\tobservations, or a dict of observation arrays.\n",
        "\n",
        "\t\tIf step_async is still doing work, that work will\n",
        "\t\tbe cancelled and step_wait() should not be called\n",
        "\t\tuntil step_async() is invoked again.\n",
        "\t\t\"\"\"\n",
        "\t\tpass\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef step_async(self, actions):\n",
        "\t\t\"\"\"\n",
        "\t\tTell all the environments to start taking a step\n",
        "\t\twith the given actions.\n",
        "\t\tCall step_wait() to get the results of the step.\n",
        "\n",
        "\t\tYou should not call this if a step_async run is\n",
        "\t\talready pending.\n",
        "\t\t\"\"\"\n",
        "\t\tpass\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef step_wait(self):\n",
        "\t\t\"\"\"\n",
        "\t\tWait for the step taken with step_async().\n",
        "\n",
        "\t\tReturns (obs, rews, dones, infos):\n",
        "\t\t - obs: an array of observations, or a dict of\n",
        "\t\t\t\tarrays of observations.\n",
        "\t\t - rews: an array of rewards\n",
        "\t\t - dones: an array of \"episode done\" booleans\n",
        "\t\t - infos: a sequence of info objects\n",
        "\t\t\"\"\"\n",
        "\t\tpass\n",
        "\n",
        "\tdef close_extras(self):\n",
        "\t\t\"\"\"\n",
        "\t\tClean up the  extra resources, beyond what's in this base class.\n",
        "\t\tOnly runs when not self.closed.\n",
        "\t\t\"\"\"\n",
        "\t\tpass\n",
        "\n",
        "\tdef close(self):\n",
        "\t\tif self.closed:\n",
        "\t\t\treturn\n",
        "\t\tif self.viewer is not None:\n",
        "\t\t\tself.viewer.close()\n",
        "\t\tself.close_extras()\n",
        "\t\tself.closed = True\n",
        "\n",
        "\tdef step(self, actions):\n",
        "\t\t\"\"\"\n",
        "\t\tStep the environments synchronously.\n",
        "\n",
        "\t\tThis is available for backwards compatibility.\n",
        "\t\t\"\"\"\n",
        "\t\tself.step_async(actions)\n",
        "\t\treturn self.step_wait()\n",
        "\n",
        "\tdef render(self, mode='human'):\n",
        "\t\timgs = self.get_images()\n",
        "\t\tbigimg = \"ARGHH\" #tile_images(imgs)\n",
        "\t\tif mode == 'human':\n",
        "\t\t\tself.get_viewer().imshow(bigimg)\n",
        "\t\t\treturn self.get_viewer().isopen\n",
        "\t\telif mode == 'rgb_array':\n",
        "\t\t\treturn bigimg\n",
        "\t\telse:\n",
        "\t\t\traise NotImplementedError\n",
        "\n",
        "\tdef get_images(self):\n",
        "\t\t\"\"\"\n",
        "\t\tReturn RGB images from each environment\n",
        "\t\t\"\"\"\n",
        "\t\traise NotImplementedError\n",
        "\n",
        "\t@property\n",
        "\tdef unwrapped(self):\n",
        "\t\tif isinstance(self, VecEnvWrapper):\n",
        "\t\t\treturn self.venv.unwrapped\n",
        "\t\telse:\n",
        "\t\t\treturn self\n",
        "\n",
        "\tdef get_viewer(self):\n",
        "\t\tif self.viewer is None:\n",
        "\t\t\tfrom gym.envs.classic_control import rendering\n",
        "\t\t\tself.viewer = rendering.SimpleImageViewer()\n",
        "\t\treturn self.viewer\n",
        "\n",
        "\t\n",
        "class VecEnvWrapper(VecEnv):\n",
        "\t\"\"\"\n",
        "\tAn environment wrapper that applies to an entire batch\n",
        "\tof environments at once.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, venv, observation_space=None, action_space=None):\n",
        "\t\tself.venv = venv\n",
        "\t\tsuper().__init__(num_envs=venv.num_envs,\n",
        "\t\t\t\t\t\tobservation_space=observation_space or venv.observation_space,\n",
        "\t\t\t\t\t\taction_space=action_space or venv.action_space)\n",
        "\n",
        "\tdef step_async(self, actions):\n",
        "\t\tself.venv.step_async(actions)\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef reset(self):\n",
        "\t\tpass\n",
        "\n",
        "\t@abstractmethod\n",
        "\tdef step_wait(self):\n",
        "\t\tpass\n",
        "\n",
        "\tdef close(self):\n",
        "\t\treturn self.venv.close()\n",
        "\n",
        "\tdef render(self, mode='human'):\n",
        "\t\treturn self.venv.render(mode=mode)\n",
        "\n",
        "\tdef get_images(self):\n",
        "\t\treturn self.venv.get_images()\n",
        "\n",
        "\tdef __getattr__(self, name):\n",
        "\t\tif name.startswith('_'):\n",
        "\t\t\traise AttributeError(\"attempted to get missing private attribute '{}'\".format(name))\n",
        "\t\treturn getattr(self.venv, name)\n",
        "\n",
        "\t\n",
        "class VecEnvObservationWrapper(VecEnvWrapper):\n",
        "\t@abstractmethod\n",
        "\tdef process(self, obs):\n",
        "\t\tpass\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tobs = self.venv.reset()\n",
        "\t\treturn self.process(obs)\n",
        "\n",
        "\tdef step_wait(self):\n",
        "\t\tobs, rews, dones, infos = self.venv.step_wait()\n",
        "\t\treturn self.process(obs), rews, dones, infos\n",
        "\n",
        "\t\n",
        "class CloudpickleWrapper(object):\n",
        "\t\"\"\"\n",
        "\tUses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, x):\n",
        "\t\tself.x = x\n",
        "\n",
        "\tdef __getstate__(self):\n",
        "\t\timport cloudpickle\n",
        "\t\treturn cloudpickle.dumps(self.x)\n",
        "\n",
        "\tdef __setstate__(self, ob):\n",
        "\t\timport pickle\n",
        "\t\tself.x = pickle.loads(ob)\n",
        "\n",
        "\t\t\n",
        "@contextlib.contextmanager\n",
        "def clear_mpi_env_vars():\n",
        "\t\"\"\"\n",
        "\tfrom mpi4py import MPI will call MPI_Init by default.  If the child process has MPI environment variables, MPI will think that the child process is an MPI process just like the parent and do bad things such as hang.\n",
        "\tThis context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing\n",
        "\tProcesses.\n",
        "\t\"\"\"\n",
        "\tremoved_environment = {}\n",
        "\tfor k, v in list(os.environ.items()):\n",
        "\t\tfor prefix in ['OMPI_', 'PMI_']:\n",
        "\t\t\tif k.startswith(prefix):\n",
        "\t\t\t\tremoved_environment[k] = v\n",
        "\t\t\t\tdel os.environ[k]\n",
        "\ttry:\n",
        "\t\tyield\n",
        "\tfinally:\n",
        "\t\tos.environ.update(removed_environment)\n",
        "\n",
        "\t\t\n",
        "class VecFrameStack(VecEnvWrapper):\n",
        "\tdef __init__(self, venv, nstack):\n",
        "\t\tself.venv = venv\n",
        "\t\tself.nstack = nstack\n",
        "\t\twos = venv.observation_space  # wrapped ob space\n",
        "\t\tlow = np.repeat(wos.low, self.nstack, axis=-1)\n",
        "\t\thigh = np.repeat(wos.high, self.nstack, axis=-1)\n",
        "\t\tself.stackedobs = np.zeros((venv.num_envs,) + low.shape, low.dtype)\n",
        "\t\tobservation_space = spaces.Box(low=low, high=high, dtype=venv.observation_space.dtype)\n",
        "\t\tVecEnvWrapper.__init__(self, venv, observation_space=observation_space)\n",
        "\n",
        "\tdef step_wait(self):\n",
        "\t\tobs, rews, news, infos = self.venv.step_wait()\n",
        "\t\tself.stackedobs = np.roll(self.stackedobs, shift=-1, axis=-1)\n",
        "\t\tfor (i, new) in enumerate(news):\n",
        "\t\t\tif new:\n",
        "\t\t\t\tself.stackedobs[i] = 0\n",
        "\t\tself.stackedobs[..., -obs.shape[-1]:] = obs\n",
        "\t\treturn self.stackedobs, rews, news, infos\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tobs = self.venv.reset()\n",
        "\t\tself.stackedobs[...] = 0\n",
        "\t\tself.stackedobs[..., -obs.shape[-1]:] = obs\n",
        "\t\treturn self.stackedobs\n",
        "\t\n",
        "class VecExtractDictObs(VecEnvObservationWrapper):\n",
        "\tdef __init__(self, venv, key):\n",
        "\t\tself.key = key\n",
        "\t\tsuper().__init__(venv=venv,\n",
        "\t\t\tobservation_space=venv.observation_space.spaces[self.key])\n",
        "\n",
        "\tdef process(self, obs):\n",
        "\t\treturn obs[self.key]\n",
        "\t\n",
        "\t\n",
        "class RunningMeanStd(object):\n",
        "\t# https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
        "\tdef __init__(self, epsilon=1e-4, shape=()):\n",
        "\t\tself.mean = np.zeros(shape, 'float64')\n",
        "\t\tself.var = np.ones(shape, 'float64')\n",
        "\t\tself.count = epsilon\n",
        "\n",
        "\tdef update(self, x):\n",
        "\t\tbatch_mean = np.mean(x, axis=0)\n",
        "\t\tbatch_var = np.var(x, axis=0)\n",
        "\t\tbatch_count = x.shape[0]\n",
        "\t\tself.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "\tdef update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "\t\tself.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
        "\t\t\tself.mean, self.var, self.count, batch_mean, batch_var, batch_count)\n",
        "\n",
        "\t\t\n",
        "def update_mean_var_count_from_moments(mean, var, count, batch_mean, batch_var, batch_count):\n",
        "\tdelta = batch_mean - mean\n",
        "\ttot_count = count + batch_count\n",
        "\n",
        "\tnew_mean = mean + delta * batch_count / tot_count\n",
        "\tm_a = var * count\n",
        "\tm_b = batch_var * batch_count\n",
        "\tM2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
        "\tnew_var = M2 / tot_count\n",
        "\tnew_count = tot_count\n",
        "\n",
        "\treturn new_mean, new_var, new_count\n",
        "\n",
        "\n",
        "class VecNormalize(VecEnvWrapper):\n",
        "\t\"\"\"\n",
        "\tA vectorized wrapper that normalizes the observations\n",
        "\tand returns from an environment.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, venv, ob=True, ret=True, clipob=10., cliprew=10., gamma=0.99, epsilon=1e-8):\n",
        "\t\tVecEnvWrapper.__init__(self, venv)\n",
        "\n",
        "\t\tself.ob_rms = RunningMeanStd(shape=self.observation_space.shape) if ob else None\n",
        "\t\tself.ret_rms = RunningMeanStd(shape=()) if ret else None\n",
        "\t\t\n",
        "\t\tself.clipob = clipob\n",
        "\t\tself.cliprew = cliprew\n",
        "\t\tself.ret = np.zeros(self.num_envs)\n",
        "\t\tself.gamma = gamma\n",
        "\t\tself.epsilon = epsilon\n",
        "\n",
        "\tdef step_wait(self):\n",
        "\t\tobs, rews, news, infos = self.venv.step_wait()\n",
        "\t\tfor i in range(len(infos)):\n",
        "\t\t\tinfos[i]['reward'] = rews[i]\n",
        "\t\tself.ret = self.ret * self.gamma + rews\n",
        "\t\tobs = self._obfilt(obs)\n",
        "\t\tif self.ret_rms:\n",
        "\t\t\tself.ret_rms.update(self.ret)\n",
        "\t\t\trews = np.clip(rews / np.sqrt(self.ret_rms.var + self.epsilon), -self.cliprew, self.cliprew)\n",
        "\t\tself.ret[news] = 0.\n",
        "\t\treturn obs, rews, news, infos\n",
        "\n",
        "\tdef _obfilt(self, obs):\n",
        "\t\tif self.ob_rms:\n",
        "\t\t\tself.ob_rms.update(obs)\n",
        "\t\t\tobs = np.clip((obs - self.ob_rms.mean) / np.sqrt(self.ob_rms.var + self.epsilon), -self.clipob, self.clipob)\n",
        "\t\t\treturn obs\n",
        "\t\telse:\n",
        "\t\t\treturn obs\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tself.ret = np.zeros(self.num_envs)\n",
        "\t\tobs = self.venv.reset()\n",
        "\t\treturn self._obfilt(obs)\n",
        "\n",
        "\n",
        "class TransposeFrame(VecEnvWrapper):\n",
        "\tdef __init__(self, env):\n",
        "\t\tsuper().__init__(venv=env)\n",
        "\t\tobs_shape = self.observation_space.shape\n",
        "\t\tself.observation_space = gym.spaces.Box(low=0, high=255, shape=(obs_shape[2], obs_shape[0], obs_shape[1]), dtype=np.float32)\n",
        "\n",
        "\tdef step_wait(self):\n",
        "\t\tobs, reward, done, info = self.venv.step_wait()\n",
        "\t\treturn obs.transpose(0,3,1,2), reward, done, info\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tobs = self.venv.reset()\n",
        "\t\treturn obs.transpose(0,3,1,2)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(VecEnvWrapper):\n",
        "\tdef __init__(self, env):\n",
        "\t\tsuper().__init__(venv=env)\n",
        "\t\tobs_shape = self.observation_space.shape\n",
        "\t\tself.observation_space = gym.spaces.Box(low=0, high=1, shape=obs_shape, dtype=np.float32)\n",
        "\n",
        "\tdef step_wait(self):\n",
        "\t\tobs, reward, done, info = self.venv.step_wait()\n",
        "\t\treturn obs/255.0, reward, done, info\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tobs = self.venv.reset()\n",
        "\t\treturn obs/255.0\n",
        "\n",
        "\n",
        "class TensorEnv(VecEnvWrapper):\n",
        "\tdef __init__(self, env):\n",
        "\t\tsuper().__init__(venv=env)\n",
        "\n",
        "\tdef step_async(self, actions):\n",
        "\t\tif isinstance(actions, torch.Tensor):\n",
        "\t\t\tactions = actions.detach().cpu().numpy()\n",
        "\t\tself.venv.step_async(actions)\n",
        "\n",
        "\tdef step_wait(self):\n",
        "\t\tobs, reward, done, info = self.venv.step_wait()\n",
        "\t\treturn torch.Tensor(obs), reward, done, info\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\tobs = self.venv.reset()\n",
        "\t\treturn torch.Tensor(obs)\n"
      ],
      "metadata": {
        "id": "CTf5pmGtTqw6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize procgen environment"
      ],
      "metadata": {
        "id": "TRoA_g_KTlD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_env(num_envs, num_levels=num_levels, gamma=gamma, env_name=env_name)"
      ],
      "metadata": {
        "id": "1Bj1GZcdTiL3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create PPO policy."
      ],
      "metadata": {
        "id": "lUYoj0BpT6sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, encoder, feature_dim, num_actions):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
        "        self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
        "\n",
        "    def act(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = x.cuda().contiguous()\n",
        "            dist, value, maxAction = self.forward(x)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "\n",
        "        return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "    def actMax(self, x):\n",
        "        with torch.no_grad():\n",
        "            x = x.cuda().contiguous()\n",
        "            dist, value, maxAction = self.forward(x)\n",
        "            action = maxAction\n",
        "            log_prob = dist.log_prob(action)\n",
        "\n",
        "        return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        logits = self.policy(x)\n",
        "        value = self.value(x).squeeze(1)\n",
        "        maxAction = logits.argmax(dim=1)\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "        return dist, value, maxAction"
      ],
      "metadata": {
        "id": "h408eLbFT_16"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model"
      ],
      "metadata": {
        "id": "FhB_3ex7UQp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_channels = 3\n",
        "\n",
        "if model == 1:\n",
        "    encoder = ImpalaModel(in_channels, nr_features)\n",
        "\n",
        "if model == 2:\n",
        "    encoder = LeakyImpalaModel(in_channels, nr_features)\n",
        "\n",
        "if model == 3:\n",
        "    encoder = FiveBlocksImpala(in_channels, nr_features)\n",
        "\n",
        "policy = Policy(encoder, nr_features, env.action_space.n)\n",
        "policy.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycnQp4zFUPYn",
        "outputId": "2225b465-4c2f-4ef6-ffe4-18e926a93fa8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Policy(\n",
              "  (encoder): ImpalaModel(\n",
              "    (block1): ImpalaBlock(\n",
              "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (res1): ResidualBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "      )\n",
              "      (res2): ResidualBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (block2): ImpalaBlock(\n",
              "      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (res1): ResidualBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "      )\n",
              "      (res2): ResidualBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (block3): ImpalaBlock(\n",
              "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (res1): ResidualBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "      )\n",
              "      (res2): ResidualBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (activation): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (fc): Linear(in_features=4096, out_features=256, bias=True)\n",
              "  )\n",
              "  (policy): Linear(in_features=256, out_features=15, bias=True)\n",
              "  (value): Linear(in_features=256, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define augmentation used during training and validation"
      ],
      "metadata": {
        "id": "NGcNQYukVZQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def stretch(frame, orgW, orgH):\n",
        "    factor = (orgW / frame.shape[1] + 1e-5, orgH / frame.shape[2] + 1e-5)\n",
        "    img = frame.float().unsqueeze(0)\n",
        "    return F.interpolate(img, scale_factor=factor)[0,:]\n",
        "\n",
        "def position(frame, orgW, orgH):\n",
        "    zeros = torch.zeros(3, orgW, orgH)\n",
        "    width = frame.shape[1]\n",
        "    height = frame.shape[2]\n",
        "    offsetX = randrange(orgW-width)\n",
        "    offsetY = randrange(orgH-height)\n",
        "    zeros[:,offsetX:offsetX+width,offsetY:offsetY+height] = frame\n",
        "    return zeros\n",
        "\n",
        "\n",
        "\n",
        "def identity(frame):\n",
        "    return frame\n",
        "\n",
        "def crop(frame):\n",
        "    orgW = frame.shape[1]\n",
        "    orgH = frame.shape[2]\n",
        "    cropsize = randrange(32) + min(orgW, orgH) - 32\n",
        "    offsetX = randrange(orgW-cropsize)\n",
        "    offsetY = randrange(orgH-cropsize)\n",
        "    cropped = frame[:,offsetX:offsetX+cropsize,offsetY:offsetY+cropsize]\n",
        "    stretched = stretch(cropped, orgW, orgH)\n",
        "    return stretched\n",
        "\n",
        "def translate(frame):\n",
        "    orgW = frame.shape[1]\n",
        "    orgH = frame.shape[2]\n",
        "    cropsize = randrange(32) + min(orgW, orgH) - 32\n",
        "    offsetX = randrange(orgW-cropsize)\n",
        "    offsetY = randrange(orgH-cropsize)\n",
        "    cropped = frame[:,offsetX:offsetX+cropsize,offsetY:offsetY+cropsize]\n",
        "    return position(cropped, orgW, orgH)\n",
        "\n",
        "def cutout(frame):\n",
        "    orgW = frame.shape[1]\n",
        "    orgH = frame.shape[2]\n",
        "    cutoutWidth = randrange(20) + 4\n",
        "    cutoutHeight = randrange(20) + 4\n",
        "    zeros = torch.zeros(3, cutoutWidth, cutoutHeight)\n",
        "    offsetX = randrange(orgW-cutoutWidth)\n",
        "    offsetY = randrange(orgH-cutoutHeight)\n",
        "    frame[:,offsetX : offsetX + cutoutWidth, offsetY : offsetY + cutoutHeight] = zeros\n",
        "    return frame\n",
        "\n",
        "def colormix(frame):\n",
        "    redWeight = 0.3 + randrange(100) / 100.0\n",
        "    greenWeight = 0.3 + randrange(100) / 100.0\n",
        "    blueWeight = 0.3 + randrange(100) / 100.0\n",
        "    frame[0] = frame[0,:] * redWeight\n",
        "    frame[1] = frame[1,:] * greenWeight\n",
        "    frame[2] = frame[2,:] * blueWeight\n",
        "    return frame\n",
        "\n",
        "AugmentationFuncArr = []\n",
        "\n",
        "RemoveModeFromRandom = False\n",
        "ModeToRemove = 0\n",
        "\n",
        "def setHoldoutAgumentation(mode):\n",
        "    global RemoveModeFromRandom\n",
        "    global ModeToRemove\n",
        "    ModeToRemove = mode\n",
        "    RemoveModeFromRandom = True\n",
        "\n",
        "def setAugmentationMode(mode, environments):\n",
        "    global AugmentationFuncArr\n",
        "    AugmentationFuncArr = []\n",
        "    for i in range(environments):\n",
        "        if mode == 0:\n",
        "            AugmentationFuncArr.append(identity)\n",
        "        elif mode == 1:\n",
        "            AugmentationFuncArr.append(crop)\n",
        "        elif mode == 2:\n",
        "            AugmentationFuncArr.append(translate)\n",
        "        elif mode == 3:\n",
        "            AugmentationFuncArr.append(cutout)\n",
        "        elif mode == 4:\n",
        "            AugmentationFuncArr.append(colormix)\n",
        "\n",
        "def setRandomAugmentationMode(environments):\n",
        "    global AugmentationFuncArr\n",
        "    global RemoveModeFromRandom\n",
        "    global ModeToRemove\n",
        "    AugmentationFuncArr = []\n",
        "    for i in range(environments):\n",
        "        mode = randrange(5)\n",
        "        if RemoveModeFromRandom:\n",
        "            while(mode == ModeToRemove):\n",
        "                mode = randrange(5)\n",
        "        if mode == 0:\n",
        "            AugmentationFuncArr.append(identity)\n",
        "        elif mode == 1:\n",
        "            AugmentationFuncArr.append(crop)\n",
        "        elif mode == 2:\n",
        "            AugmentationFuncArr.append(translate)\n",
        "        elif mode == 3:\n",
        "            AugmentationFuncArr.append(cutout)\n",
        "        elif mode == 4:\n",
        "            AugmentationFuncArr.append(colormix)\n",
        "\n",
        "def augment(obs):\n",
        "    for i in range(obs.shape[0]):\n",
        "        obs[i] = AugmentationFuncArr[i % len(AugmentationFuncArr)](obs[i])\n",
        "    return obs\n",
        "\n",
        "def testCrop():\n",
        "    env = make_env(4, num_levels=10, gamma=0.999, env_name='coinrun')\n",
        "    obs = env.reset()\n",
        "    for i in range(obs.shape[0]):\n",
        "        frame = (obs[0,:]*255.)\n",
        "        imageio.imsave(\"crop\" + str(i) + \".png\",crop(frame).T.byte())\n",
        "\n",
        "def testTranslate():\n",
        "    env = make_env(4, num_levels=10, gamma=0.999, env_name='coinrun')\n",
        "    obs = env.reset()\n",
        "    for i in range(obs.shape[0]):\n",
        "        frame = (obs[0,:]*255.)\n",
        "        imageio.imsave(\"translate\" + str(i) + \".png\",translate(frame).T.byte())\n",
        "\n",
        "def testCutout():\n",
        "    env = make_env(4, num_levels=10, gamma=0.999, env_name='coinrun')\n",
        "    obs = env.reset()\n",
        "    for i in range(obs.shape[0]):\n",
        "        frame = (obs[0,:]*255.)\n",
        "        imageio.imsave(\"cutout\" + str(i) + \".png\",cutout(frame).T.byte())\n",
        "\n",
        "def testColorMix():\n",
        "    env = make_env(4, num_levels=10, gamma=0.999, env_name='coinrun')\n",
        "    obs = env.reset()\n",
        "    for i in range(obs.shape[0]):\n",
        "        frame = (obs[0,:]*255.)\n",
        "        imageio.imsave(\"colorMix\" + str(i) + \".png\",colormix(frame).T.byte())\n"
      ],
      "metadata": {
        "id": "aix3pPAJVjq0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize all the requisites for the training.\n",
        "\n"
      ],
      "metadata": {
        "id": "2bZqzkOaU2Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer\n",
        "# these are reasonable values but probably not optimal\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n",
        "\n",
        "# Define temporary storage\n",
        "# we use this to collect transitions during each iteration\n",
        "storage = Storage(\n",
        "    env.observation_space.shape,\n",
        "    num_steps,\n",
        "    num_envs,\n",
        "    gamma\n",
        ")\n",
        "\n",
        "def resetAugmentationMode():\n",
        "    if (randomAugmentation):\n",
        "        setRandomAugmentationMode(num_envs)\n",
        "    else:\n",
        "        setAugmentationMode(augmentationMode, num_envs)\n",
        "\n",
        "if useHoldoutAugmentation:\n",
        "    setHoldoutAgumentation(augmentationModeValidation)\n",
        "\n",
        "\n",
        "def evaluate(step, testEnv, testEnvAugmentationMode = 0):\n",
        "    setAugmentationMode(testEnvAugmentationMode, num_envs)\n",
        "    # Make evaluation environment\n",
        "    startlvl = 0\n",
        "    numlevels = num_levels\n",
        "    if testEnv:\n",
        "        startlvl = num_levels\n",
        "        numlevels = 100000\n",
        "    eval_env = make_env(num_envs, start_level=startlvl, num_levels=numlevels, gamma=gamma, env_name=env_name)\n",
        "    obs = eval_env.reset()\n",
        "\n",
        "\n",
        "    total_reward = []\n",
        "\n",
        "    # Evaluate policy\n",
        "    policy.eval()\n",
        "    for _ in range(2048):\n",
        "        # Use policy\n",
        "        action, log_prob, value = policy.actMax(augment(obs))\n",
        "\n",
        "        # Take step in environment\n",
        "        obs, reward, done, info = eval_env.step(action)\n",
        "        total_reward.append(torch.Tensor(reward))\n",
        "\n",
        "    # Calculate average return\n",
        "    total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
        "    if testEnv:\n",
        "        resetAugmentationMode()\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "resetAugmentationMode()"
      ],
      "metadata": {
        "id": "mWqA5Mo9U34F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training\n",
        "obs = augment(env.reset())\n",
        "# obs = augment(obs)\n",
        "step = 0\n",
        "lastEval = step\n",
        "while step < total_steps:\n",
        "    # Use policy to collect data for num_steps steps\n",
        "    policy.eval()\n",
        "    for _ in range(num_steps):\n",
        "        # Use policy\n",
        "        action, log_prob, value = policy.act(obs)\n",
        "\n",
        "        # Take step in environment\n",
        "        next_obs, reward, done, info = env.step(action)\n",
        "        # Update augmentation mode if we have random augmentations\n",
        "        if (randomAugmentation and randrange(3) == 0):\n",
        "            setRandomAugmentationMode(num_envs)\n",
        "\n",
        "        # Store data\n",
        "        storage.store(obs, action, reward, done, info, log_prob, value)\n",
        "\n",
        "        # Update current observation\n",
        "        obs = augment(next_obs)\n",
        "\n",
        "    # Add the last observation to collected data\n",
        "    _, _, value = policy.act(obs)\n",
        "    storage.store_last(obs, value)\n",
        "\n",
        "    # Compute return and advantage\n",
        "    storage.compute_return_advantage()\n",
        "\n",
        "    # Optimize policy\n",
        "    policy.train()\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # Iterate over batches of transitions\n",
        "        generator = storage.get_generator(batch_size)\n",
        "        for batch in generator:\n",
        "            b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
        "\n",
        "            # Get current policy outputs\n",
        "            new_dist, new_value, _ = policy(augment(b_obs))\n",
        "            new_log_prob = new_dist.log_prob(b_action)\n",
        "\n",
        "            ratio = (new_log_prob - b_log_prob).exp()\n",
        "            surr1 = ratio * b_advantage\n",
        "            surr2 = torch.clamp(ratio, 1.0 - eps, 1.0 + eps) * b_advantage\n",
        "\n",
        "            # Clipped policy objective\n",
        "            pi_loss = - torch.min(surr1, surr2).mean()\n",
        "            # Clipped value function objective\n",
        "            value_loss = (new_value - b_returns).pow(2).mean()\n",
        "\n",
        "            # Entropy loss\n",
        "            entropy_loss = torch.mean(torch.exp(new_log_prob) * new_log_prob)\n",
        "\n",
        "            # Backpropagate losses\n",
        "            loss = pi_loss + value_coef * value_loss + entropy_coef * entropy_loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradients\n",
        "            torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
        "\n",
        "            # Update policy\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    # Update stats\n",
        "    step += num_envs * num_steps\n",
        "    if ((lastEval < 1e6 and (step - lastEval) >= eval_frequency_min) or ((step - lastEval) >= eval_frequency_max) or step >= total_steps):\n",
        "        trainScore = evaluate(step, False, augmentationModeValidation)\n",
        "        testScore = evaluate(step, True, augmentationModeValidation)\n",
        "        print(f'Step: {step}\\t({trainScore},{testScore})')\n",
        "        lastEval = step\n",
        "\n",
        "print('Completed training!')\n",
        "torch.save(policy.state_dict, 'checkpoint.pt')"
      ],
      "metadata": {
        "id": "1ddpFdkPVBTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate video of the model ."
      ],
      "metadata": {
        "id": "sR25wLDmVFjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "\n",
        "# Make evaluation environment\n",
        "eval_env = make_env(num_envs, start_level=num_levels, num_levels=num_levels, gamma=gamma, env_name=env_name)\n",
        "obs = eval_env.reset()\n",
        "\n",
        "frames = []\n",
        "total_reward = []\n",
        "\n",
        "# Evaluate policy\n",
        "policy.eval()\n",
        "for _ in range(2048):\n",
        "    # Use policy\n",
        "    action, log_prob, value = policy.actMax(obs)\n",
        "\n",
        "    # Take step in environment\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "\n",
        "    total_reward.append(torch.Tensor(reward))\n",
        "\n",
        "    # Render environment and store\n",
        "    frame = (torch.Tensor(eval_env.render(mode='rgb_array')) * 255.).byte()\n",
        "    frames.append(frame)\n",
        "\n",
        "# Calculate average return\n",
        "total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
        "print('Average return:', total_reward)\n",
        "# Save frames as video\n",
        "frames = torch.stack(frames)\n",
        "imageio.mimsave('vid.mp4', frames, fps=25)\n"
      ],
      "metadata": {
        "id": "24OqkWmDVSvw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}